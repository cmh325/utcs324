{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "Let's draw some pictures today.\n",
    "\n",
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "# Colors to visualize the labeling\n",
    "COLORS = np.array([(0,0,0), (255,0,0), (0,255,0), (255,255,0), (0,0,255), (255,255,255)], dtype=np.uint8)\n",
    "CROP_SIZE = 64\n",
    "\n",
    "def parser(record):\n",
    "    # Parse the TF record\n",
    "    parsed = tf.parse_single_example(record, features={\n",
    "        'height': tf.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'label_raw': tf.FixedLenFeature([], tf.string)\n",
    "    })\n",
    "    # Load the data and format it\n",
    "    H = tf.cast(parsed['height'], tf.int32)\n",
    "    W = tf.cast(parsed['width'], tf.int32)\n",
    "    image = tf.reshape(tf.decode_raw(parsed[\"image_raw\"], tf.uint8), [H,W,3])\n",
    "    label = tf.reshape(tf.decode_raw(parsed[\"label_raw\"], tf.uint8), [H,W])\n",
    "    \n",
    "    ## Data augmentation\n",
    "    # Stack the image and labels to make sure the same operations are applied\n",
    "    data = tf.concat([image, label[:,:,None]], axis=-1)\n",
    "    \n",
    "    # TODO: Apply the data augmentation (you should both crop the images randomly and flip them)\n",
    "    data = tf.random_crop(data, [CROP_SIZE, CROP_SIZE, 4])\n",
    "    data = tf.image.random_flip_left_right(data)\n",
    "    \n",
    "    return data[:,:,:-1], data[:,:,-1]\n",
    "\n",
    "def load_dataset(tfrecord):\n",
    "    # Load the dataset\n",
    "    dataset = tf.contrib.data.TFRecordDataset(tfrecord)\n",
    "\n",
    "    # Parse the tf record entries\n",
    "    dataset = dataset.map(parser, num_threads=8, output_buffer_size=1024)\n",
    "\n",
    "    # Shuffle the data, batch it and run this for multiple epochs\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset\n",
    "\n",
    "# We still have 6 classes\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define your convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of variables used  602359\n"
     ]
    }
   ],
   "source": [
    "# Create a new log directory (if you run low on disk space you can either disable this or delete old logs)\n",
    "# run: `tensorboard --logdir log` to see all the nice summaries\n",
    "for n_model in range(1000):\n",
    "    LOG_DIR = 'log/model_%d'%n_model\n",
    "    from os import path\n",
    "    if not path.exists(LOG_DIR):\n",
    "        break\n",
    "\n",
    "# Lets clear the tensorflow graph, so that you don't have to restart the notebook every time you change the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "TF_COLORS = tf.constant(COLORS)\n",
    "\n",
    "train_data = load_dataset('train.tfrecord')\n",
    "valid_data = load_dataset('valid.tfrecord')\n",
    "\n",
    "# Create an iterator for the datasets\n",
    "# The iterator allows us to quickly switch between training and validataion\n",
    "iterator = tf.contrib.data.Iterator.from_structure(train_data.output_types, ((None,None,None,3), (None,None,None)))\n",
    "\n",
    "# and fetch the next images from the dataset (every time next_image is evaluated a new image set of 32 images is returned)\n",
    "next_image, next_label = iterator.get_next()\n",
    "\n",
    "# Define operations that switch between train and valid\n",
    "switch_train_op = iterator.make_initializer(train_data)\n",
    "switch_valid_op = iterator.make_initializer(valid_data)\n",
    "\n",
    "# Convert the input\n",
    "image = tf.cast(next_image, tf.float32)\n",
    "label = tf.cast(next_label, tf.int32)\n",
    "\n",
    "# Define the input\n",
    "label = tf.identity(label, name='label')\n",
    "# Whiten the one hot\n",
    "one_hot_label = tf.one_hot(label, num_classes) - np.array([ 0.66839117, 0.00382957, 0.00092516, 0.00345217, 0.00339063, 0.3200113 ])[None,None,None,:]\n",
    "\n",
    "# Whiten the image\n",
    "white_image = (image - 100.) / 72.\n",
    "\n",
    "# Let's upsample an image using the label map as a guidance\n",
    "image_lr = tf.layers.average_pooling2d(image, 5, 4, padding='SAME')\n",
    "image_lr = tf.identity(image_lr, name='image_lr')\n",
    "white_lr = (image_lr - 100.) / 72.\n",
    "\n",
    "# TODO: Define your convnet here ()\n",
    "upsampled = tf.image.resize_images(white_lr, tf.shape(label)[1:3])\n",
    "\n",
    "C0 = 25\n",
    "D = 5\n",
    "h = tf.concat([one_hot_label, upsampled], axis=-1)\n",
    "hs = []\n",
    "for i in range(D):\n",
    "    hs.append(h)\n",
    "    h = tf.contrib.layers.conv2d(h, int(C0*1.5**i), (3,3), stride=2, scope='conv%d'%(i+1))\n",
    "    h = tf.concat([h, tf.image.resize_images(white_lr,  tf.shape(h)[1:3])], axis=-1)\n",
    "\n",
    "for i in range(D)[::-1]:\n",
    "    h = tf.contrib.layers.conv2d_transpose(h, int(C0*1.5**i), (3,3), stride=2, scope='upconv%d'%(i+1))\n",
    "    h = tf.concat([h, hs[i]], axis=-1)\n",
    "h = tf.contrib.layers.conv2d(h, C0, (1,1), scope='fc1')\n",
    "h = tf.contrib.layers.conv2d(h, 3, (1,1), scope='cls', activation_fn=None)\n",
    "\n",
    "h = h + upsampled\n",
    "\n",
    "# Let's compute the output labeling\n",
    "output = tf.cast(tf.clip_by_value(72.*h + 100., 0, 255), tf.uint8, name='output')\n",
    "\n",
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.abs(white_image - h))\n",
    "\n",
    "# Let's weight the regularization loss down, otherwise it will hurt the model performance\n",
    "# You can tune this weight if you wish\n",
    "regularization_loss = tf.losses.get_regularization_loss()\n",
    "total_loss = loss + 1e-6 * regularization_loss\n",
    "\n",
    "# Adam will likely converge much faster than SGD for this assignment.\n",
    "optimizer = tf.train.AdamOptimizer(0.001, 0.9, 0.999)\n",
    "# optimizer = tf.train.MomentumOptimizer(0.001, 0.9)\n",
    "\n",
    "# use that optimizer on your loss function (control_dependencies makes sure any \n",
    "# batch_norm parameters are properly updated)\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    opt = optimizer.minimize(total_loss)\n",
    "\n",
    "# Let's define some summaries for tensorboard\n",
    "colored_label = tf.gather_nd(TF_COLORS, label[:,:,:,None])\n",
    "tf.summary.image('image', next_image, max_outputs=3)\n",
    "tf.summary.image('label', colored_label, max_outputs=3)\n",
    "tf.summary.image('output', output, max_outputs=3)\n",
    "tf.summary.image('image_lr', image_lr, max_outputs=3)\n",
    "tf.summary.scalar('loss', tf.placeholder(tf.float32, name='loss'))\n",
    "tf.summary.scalar('val_loss', tf.placeholder(tf.float32, name='val_loss'))\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR, tf.get_default_graph())\n",
    "\n",
    "# Let's compute the model size\n",
    "print( \"Total number of variables used \", np.sum([v.get_shape().num_elements() for v in tf.trainable_variables()]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training\n",
    "\n",
    "Training might take up to 20 min depending on your architecture (and if you have a GPU or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0] Loss: 0.311  \t  val loss A.: 0.260\n",
      "[  1] Loss: 0.238  \t  val loss A.: 0.227\n",
      "[  2] Loss: 0.226  \t  val loss A.: 0.188\n",
      "[  3] Loss: 0.231  \t  val loss A.: 0.166\n",
      "[  4] Loss: 0.233  \t  val loss A.: 0.200\n",
      "[  5] Loss: 0.213  \t  val loss A.: 0.158\n",
      "[  6] Loss: 0.225  \t  val loss A.: 0.164\n",
      "[  7] Loss: 0.213  \t  val loss A.: 0.212\n",
      "[  8] Loss: 0.211  \t  val loss A.: 0.180\n",
      "[  9] Loss: 0.211  \t  val loss A.: 0.178\n",
      "[ 10] Loss: 0.213  \t  val loss A.: 0.172\n",
      "[ 11] Loss: 0.198  \t  val loss A.: 0.166\n",
      "[ 12] Loss: 0.194  \t  val loss A.: 0.155\n",
      "[ 13] Loss: 0.204  \t  val loss A.: 0.163\n",
      "[ 14] Loss: 0.199  \t  val loss A.: 0.172\n",
      "[ 15] Loss: 0.179  \t  val loss A.: 0.187\n",
      "[ 16] Loss: 0.194  \t  val loss A.: 0.176\n",
      "[ 17] Loss: 0.183  \t  val loss A.: 0.170\n",
      "[ 18] Loss: 0.188  \t  val loss A.: 0.156\n",
      "[ 19] Loss: 0.176  \t  val loss A.: 0.174\n",
      "[ 20] Loss: 0.188  \t  val loss A.: 0.174\n",
      "[ 21] Loss: 0.172  \t  val loss A.: 0.198\n",
      "[ 22] Loss: 0.182  \t  val loss A.: 0.156\n",
      "[ 23] Loss: 0.194  \t  val loss A.: 0.121\n",
      "[ 24] Loss: 0.179  \t  val loss A.: 0.183\n",
      "[ 25] Loss: 0.195  \t  val loss A.: 0.143\n",
      "[ 26] Loss: 0.177  \t  val loss A.: 0.153\n",
      "[ 27] Loss: 0.171  \t  val loss A.: 0.170\n",
      "[ 28] Loss: 0.183  \t  val loss A.: 0.172\n",
      "[ 29] Loss: 0.168  \t  val loss A.: 0.142\n",
      "[ 30] Loss: 0.169  \t  val loss A.: 0.154\n",
      "[ 31] Loss: 0.182  \t  val loss A.: 0.169\n",
      "[ 32] Loss: 0.180  \t  val loss A.: 0.138\n",
      "[ 33] Loss: 0.177  \t  val loss A.: 0.124\n",
      "[ 34] Loss: 0.173  \t  val loss A.: 0.156\n",
      "[ 35] Loss: 0.168  \t  val loss A.: 0.115\n",
      "[ 36] Loss: 0.167  \t  val loss A.: 0.152\n",
      "[ 37] Loss: 0.171  \t  val loss A.: 0.155\n",
      "[ 38] Loss: 0.176  \t  val loss A.: 0.151\n",
      "[ 39] Loss: 0.169  \t  val loss A.: 0.162\n",
      "[ 40] Loss: 0.168  \t  val loss A.: 0.138\n",
      "[ 41] Loss: 0.178  \t  val loss A.: 0.153\n",
      "[ 42] Loss: 0.171  \t  val loss A.: 0.168\n",
      "[ 43] Loss: 0.169  \t  val loss A.: 0.146\n",
      "[ 44] Loss: 0.173  \t  val loss A.: 0.145\n",
      "[ 45] Loss: 0.173  \t  val loss A.: 0.140\n",
      "[ 46] Loss: 0.161  \t  val loss A.: 0.126\n",
      "[ 47] Loss: 0.166  \t  val loss A.: 0.159\n",
      "[ 48] Loss: 0.172  \t  val loss A.: 0.169\n",
      "[ 49] Loss: 0.161  \t  val loss A.: 0.128\n",
      "[ 50] Loss: 0.172  \t  val loss A.: 0.127\n",
      "[ 51] Loss: 0.162  \t  val loss A.: 0.154\n",
      "[ 52] Loss: 0.171  \t  val loss A.: 0.157\n",
      "[ 53] Loss: 0.166  \t  val loss A.: 0.145\n",
      "[ 54] Loss: 0.168  \t  val loss A.: 0.161\n",
      "[ 55] Loss: 0.167  \t  val loss A.: 0.115\n",
      "[ 56] Loss: 0.172  \t  val loss A.: 0.153\n",
      "[ 57] Loss: 0.163  \t  val loss A.: 0.140\n",
      "[ 58] Loss: 0.168  \t  val loss A.: 0.140\n",
      "[ 59] Loss: 0.159  \t  val loss A.: 0.150\n",
      "[ 60] Loss: 0.168  \t  val loss A.: 0.148\n",
      "[ 61] Loss: 0.166  \t  val loss A.: 0.159\n",
      "[ 62] Loss: 0.174  \t  val loss A.: 0.160\n",
      "[ 63] Loss: 0.159  \t  val loss A.: 0.159\n",
      "[ 64] Loss: 0.165  \t  val loss A.: 0.151\n",
      "[ 65] Loss: 0.157  \t  val loss A.: 0.156\n",
      "[ 66] Loss: 0.151  \t  val loss A.: 0.153\n",
      "[ 67] Loss: 0.158  \t  val loss A.: 0.139\n",
      "[ 68] Loss: 0.160  \t  val loss A.: 0.127\n",
      "[ 69] Loss: 0.167  \t  val loss A.: 0.135\n",
      "[ 70] Loss: 0.160  \t  val loss A.: 0.128\n",
      "[ 71] Loss: 0.158  \t  val loss A.: 0.178\n",
      "[ 72] Loss: 0.162  \t  val loss A.: 0.138\n",
      "[ 73] Loss: 0.159  \t  val loss A.: 0.118\n",
      "[ 74] Loss: 0.163  \t  val loss A.: 0.176\n",
      "[ 75] Loss: 0.164  \t  val loss A.: 0.116\n",
      "[ 76] Loss: 0.166  \t  val loss A.: 0.156\n",
      "[ 77] Loss: 0.158  \t  val loss A.: 0.156\n",
      "[ 78] Loss: 0.165  \t  val loss A.: 0.130\n",
      "[ 79] Loss: 0.158  \t  val loss A.: 0.145\n",
      "[ 80] Loss: 0.160  \t  val loss A.: 0.135\n",
      "[ 81] Loss: 0.157  \t  val loss A.: 0.147\n",
      "[ 82] Loss: 0.158  \t  val loss A.: 0.141\n",
      "[ 83] Loss: 0.156  \t  val loss A.: 0.137\n",
      "[ 84] Loss: 0.154  \t  val loss A.: 0.124\n",
      "[ 85] Loss: 0.158  \t  val loss A.: 0.140\n",
      "[ 86] Loss: 0.159  \t  val loss A.: 0.139\n",
      "[ 87] Loss: 0.150  \t  val loss A.: 0.136\n",
      "[ 88] Loss: 0.150  \t  val loss A.: 0.118\n",
      "[ 89] Loss: 0.153  \t  val loss A.: 0.117\n",
      "[ 90] Loss: 0.153  \t  val loss A.: 0.131\n",
      "[ 91] Loss: 0.146  \t  val loss A.: 0.127\n",
      "[ 92] Loss: 0.149  \t  val loss A.: 0.162\n",
      "[ 93] Loss: 0.152  \t  val loss A.: 0.157\n",
      "[ 94] Loss: 0.157  \t  val loss A.: 0.120\n",
      "[ 95] Loss: 0.158  \t  val loss A.: 0.117\n",
      "[ 96] Loss: 0.156  \t  val loss A.: 0.149\n",
      "[ 97] Loss: 0.163  \t  val loss A.: 0.114\n",
      "[ 98] Loss: 0.155  \t  val loss A.: 0.148\n",
      "[ 99] Loss: 0.148  \t  val loss A.: 0.142\n",
      "[100] Loss: 0.155  \t  val loss A.: 0.144\n",
      "[101] Loss: 0.152  \t  val loss A.: 0.128\n",
      "[102] Loss: 0.156  \t  val loss A.: 0.134\n",
      "[103] Loss: 0.152  \t  val loss A.: 0.138\n",
      "[104] Loss: 0.155  \t  val loss A.: 0.147\n",
      "[105] Loss: 0.157  \t  val loss A.: 0.096\n",
      "[106] Loss: 0.155  \t  val loss A.: 0.142\n",
      "[107] Loss: 0.143  \t  val loss A.: 0.150\n",
      "[108] Loss: 0.157  \t  val loss A.: 0.114\n",
      "[109] Loss: 0.150  \t  val loss A.: 0.135\n",
      "[110] Loss: 0.153  \t  val loss A.: 0.135\n",
      "[111] Loss: 0.142  \t  val loss A.: 0.125\n",
      "[112] Loss: 0.150  \t  val loss A.: 0.124\n",
      "[113] Loss: 0.147  \t  val loss A.: 0.141\n",
      "[114] Loss: 0.154  \t  val loss A.: 0.136\n",
      "[115] Loss: 0.144  \t  val loss A.: 0.121\n",
      "[116] Loss: 0.153  \t  val loss A.: 0.144\n",
      "[117] Loss: 0.147  \t  val loss A.: 0.121\n",
      "[118] Loss: 0.148  \t  val loss A.: 0.120\n",
      "[119] Loss: 0.145  \t  val loss A.: 0.129\n",
      "[120] Loss: 0.146  \t  val loss A.: 0.109\n",
      "[121] Loss: 0.151  \t  val loss A.: 0.127\n",
      "[122] Loss: 0.144  \t  val loss A.: 0.120\n",
      "[123] Loss: 0.134  \t  val loss A.: 0.118\n",
      "[124] Loss: 0.137  \t  val loss A.: 0.132\n",
      "[125] Loss: 0.140  \t  val loss A.: 0.138\n",
      "[126] Loss: 0.145  \t  val loss A.: 0.125\n",
      "[127] Loss: 0.139  \t  val loss A.: 0.136\n",
      "[128] Loss: 0.144  \t  val loss A.: 0.158\n",
      "[129] Loss: 0.150  \t  val loss A.: 0.124\n",
      "[130] Loss: 0.145  \t  val loss A.: 0.126\n",
      "[131] Loss: 0.147  \t  val loss A.: 0.133\n",
      "[132] Loss: 0.145  \t  val loss A.: 0.140\n",
      "[133] Loss: 0.140  \t  val loss A.: 0.111\n",
      "[134] Loss: 0.142  \t  val loss A.: 0.128\n",
      "[135] Loss: 0.146  \t  val loss A.: 0.141\n",
      "[136] Loss: 0.143  \t  val loss A.: 0.129\n",
      "[137] Loss: 0.145  \t  val loss A.: 0.103\n",
      "[138] Loss: 0.142  \t  val loss A.: 0.120\n",
      "[139] Loss: 0.149  \t  val loss A.: 0.125\n",
      "[140] Loss: 0.145  \t  val loss A.: 0.133\n",
      "[141] Loss: 0.150  \t  val loss A.: 0.114\n",
      "[142] Loss: 0.151  \t  val loss A.: 0.146\n",
      "[143] Loss: 0.154  \t  val loss A.: 0.150\n",
      "[144] Loss: 0.142  \t  val loss A.: 0.109\n",
      "[145] Loss: 0.148  \t  val loss A.: 0.124\n",
      "[146] Loss: 0.136  \t  val loss A.: 0.115\n",
      "[147] Loss: 0.144  \t  val loss A.: 0.123\n",
      "[148] Loss: 0.144  \t  val loss A.: 0.107\n",
      "[149] Loss: 0.145  \t  val loss A.: 0.127\n",
      "[150] Loss: 0.141  \t  val loss A.: 0.137\n",
      "[151] Loss: 0.146  \t  val loss A.: 0.115\n",
      "[152] Loss: 0.144  \t  val loss A.: 0.145\n",
      "[153] Loss: 0.141  \t  val loss A.: 0.115\n",
      "[154] Loss: 0.143  \t  val loss A.: 0.145\n",
      "[155] Loss: 0.135  \t  val loss A.: 0.114\n",
      "[156] Loss: 0.144  \t  val loss A.: 0.114\n",
      "[157] Loss: 0.136  \t  val loss A.: 0.130\n",
      "[158] Loss: 0.137  \t  val loss A.: 0.119\n",
      "[159] Loss: 0.143  \t  val loss A.: 0.145\n",
      "[160] Loss: 0.142  \t  val loss A.: 0.107\n",
      "[161] Loss: 0.136  \t  val loss A.: 0.111\n",
      "[162] Loss: 0.135  \t  val loss A.: 0.112\n",
      "[163] Loss: 0.129  \t  val loss A.: 0.136\n",
      "[164] Loss: 0.139  \t  val loss A.: 0.126\n",
      "[165] Loss: 0.140  \t  val loss A.: 0.137\n",
      "[166] Loss: 0.138  \t  val loss A.: 0.115\n",
      "[167] Loss: 0.142  \t  val loss A.: 0.116\n",
      "[168] Loss: 0.148  \t  val loss A.: 0.118\n",
      "[169] Loss: 0.136  \t  val loss A.: 0.111\n",
      "[170] Loss: 0.137  \t  val loss A.: 0.115\n",
      "[171] Loss: 0.137  \t  val loss A.: 0.130\n",
      "[172] Loss: 0.139  \t  val loss A.: 0.125\n",
      "[173] Loss: 0.138  \t  val loss A.: 0.125\n",
      "[174] Loss: 0.137  \t  val loss A.: 0.132\n",
      "[175] Loss: 0.138  \t  val loss A.: 0.142\n",
      "[176] Loss: 0.138  \t  val loss A.: 0.100\n",
      "[177] Loss: 0.141  \t  val loss A.: 0.132\n",
      "[178] Loss: 0.143  \t  val loss A.: 0.141\n",
      "[179] Loss: 0.140  \t  val loss A.: 0.122\n",
      "[180] Loss: 0.141  \t  val loss A.: 0.132\n",
      "[181] Loss: 0.145  \t  val loss A.: 0.128\n",
      "[182] Loss: 0.147  \t  val loss A.: 0.130\n",
      "[183] Loss: 0.139  \t  val loss A.: 0.103\n",
      "[184] Loss: 0.130  \t  val loss A.: 0.126\n",
      "[185] Loss: 0.138  \t  val loss A.: 0.120\n",
      "[186] Loss: 0.135  \t  val loss A.: 0.116\n",
      "[187] Loss: 0.131  \t  val loss A.: 0.127\n",
      "[188] Loss: 0.129  \t  val loss A.: 0.121\n",
      "[189] Loss: 0.143  \t  val loss A.: 0.099\n",
      "[190] Loss: 0.138  \t  val loss A.: 0.126\n",
      "[191] Loss: 0.135  \t  val loss A.: 0.103\n",
      "[192] Loss: 0.139  \t  val loss A.: 0.107\n",
      "[193] Loss: 0.133  \t  val loss A.: 0.117\n",
      "[194] Loss: 0.137  \t  val loss A.: 0.120\n",
      "[195] Loss: 0.142  \t  val loss A.: 0.116\n",
      "[196] Loss: 0.136  \t  val loss A.: 0.103\n",
      "[197] Loss: 0.131  \t  val loss A.: 0.133\n",
      "[198] Loss: 0.135  \t  val loss A.: 0.114\n",
      "[199] Loss: 0.138  \t  val loss A.: 0.109\n",
      "[200] Loss: 0.140  \t  val loss A.: 0.120\n",
      "[201] Loss: 0.132  \t  val loss A.: 0.116\n",
      "[202] Loss: 0.141  \t  val loss A.: 0.098\n",
      "[203] Loss: 0.132  \t  val loss A.: 0.124\n",
      "[204] Loss: 0.141  \t  val loss A.: 0.118\n",
      "[205] Loss: 0.140  \t  val loss A.: 0.104\n",
      "[206] Loss: 0.137  \t  val loss A.: 0.123\n",
      "[207] Loss: 0.135  \t  val loss A.: 0.107\n",
      "[208] Loss: 0.132  \t  val loss A.: 0.106\n",
      "[209] Loss: 0.139  \t  val loss A.: 0.109\n",
      "[210] Loss: 0.135  \t  val loss A.: 0.148\n",
      "[211] Loss: 0.133  \t  val loss A.: 0.121\n",
      "[212] Loss: 0.139  \t  val loss A.: 0.118\n",
      "[213] Loss: 0.138  \t  val loss A.: 0.125\n",
      "[214] Loss: 0.129  \t  val loss A.: 0.122\n",
      "[215] Loss: 0.135  \t  val loss A.: 0.123\n",
      "[216] Loss: 0.140  \t  val loss A.: 0.123\n",
      "[217] Loss: 0.130  \t  val loss A.: 0.123\n",
      "[218] Loss: 0.131  \t  val loss A.: 0.119\n",
      "[219] Loss: 0.131  \t  val loss A.: 0.128\n",
      "[220] Loss: 0.135  \t  val loss A.: 0.124\n",
      "[221] Loss: 0.140  \t  val loss A.: 0.102\n",
      "[222] Loss: 0.129  \t  val loss A.: 0.116\n",
      "[223] Loss: 0.129  \t  val loss A.: 0.113\n",
      "[224] Loss: 0.132  \t  val loss A.: 0.125\n",
      "[225] Loss: 0.131  \t  val loss A.: 0.133\n",
      "[226] Loss: 0.137  \t  val loss A.: 0.103\n",
      "[227] Loss: 0.133  \t  val loss A.: 0.115\n",
      "[228] Loss: 0.127  \t  val loss A.: 0.132\n",
      "[229] Loss: 0.139  \t  val loss A.: 0.106\n",
      "[230] Loss: 0.129  \t  val loss A.: 0.124\n",
      "[231] Loss: 0.136  \t  val loss A.: 0.127\n",
      "[232] Loss: 0.131  \t  val loss A.: 0.110\n",
      "[233] Loss: 0.136  \t  val loss A.: 0.136\n",
      "[234] Loss: 0.136  \t  val loss A.: 0.100\n",
      "[235] Loss: 0.134  \t  val loss A.: 0.114\n",
      "[236] Loss: 0.127  \t  val loss A.: 0.102\n",
      "[237] Loss: 0.130  \t  val loss A.: 0.118\n",
      "[238] Loss: 0.141  \t  val loss A.: 0.114\n",
      "[239] Loss: 0.134  \t  val loss A.: 0.133\n",
      "[240] Loss: 0.129  \t  val loss A.: 0.131\n",
      "[241] Loss: 0.134  \t  val loss A.: 0.124\n",
      "[242] Loss: 0.126  \t  val loss A.: 0.120\n",
      "[243] Loss: 0.139  \t  val loss A.: 0.131\n",
      "[244] Loss: 0.133  \t  val loss A.: 0.104\n",
      "[245] Loss: 0.127  \t  val loss A.: 0.139\n",
      "[246] Loss: 0.138  \t  val loss A.: 0.116\n",
      "[247] Loss: 0.125  \t  val loss A.: 0.126\n",
      "[248] Loss: 0.131  \t  val loss A.: 0.122\n",
      "[249] Loss: 0.139  \t  val loss A.: 0.103\n",
      "[250] Loss: 0.130  \t  val loss A.: 0.103\n",
      "[251] Loss: 0.130  \t  val loss A.: 0.126\n",
      "[252] Loss: 0.131  \t  val loss A.: 0.120\n",
      "[253] Loss: 0.136  \t  val loss A.: 0.110\n",
      "[254] Loss: 0.130  \t  val loss A.: 0.129\n",
      "[255] Loss: 0.132  \t  val loss A.: 0.127\n",
      "[256] Loss: 0.129  \t  val loss A.: 0.155\n",
      "[257] Loss: 0.129  \t  val loss A.: 0.118\n",
      "[258] Loss: 0.132  \t  val loss A.: 0.114\n",
      "[259] Loss: 0.127  \t  val loss A.: 0.104\n",
      "[260] Loss: 0.129  \t  val loss A.: 0.120\n",
      "[261] Loss: 0.136  \t  val loss A.: 0.108\n",
      "[262] Loss: 0.122  \t  val loss A.: 0.121\n",
      "[263] Loss: 0.138  \t  val loss A.: 0.104\n",
      "[264] Loss: 0.132  \t  val loss A.: 0.123\n",
      "[265] Loss: 0.136  \t  val loss A.: 0.105\n",
      "[266] Loss: 0.142  \t  val loss A.: 0.103\n",
      "[267] Loss: 0.141  \t  val loss A.: 0.120\n",
      "[268] Loss: 0.128  \t  val loss A.: 0.105\n",
      "[269] Loss: 0.132  \t  val loss A.: 0.110\n",
      "[270] Loss: 0.128  \t  val loss A.: 0.126\n",
      "[271] Loss: 0.136  \t  val loss A.: 0.126\n",
      "[272] Loss: 0.125  \t  val loss A.: 0.121\n",
      "[273] Loss: 0.139  \t  val loss A.: 0.127\n",
      "[274] Loss: 0.133  \t  val loss A.: 0.104\n",
      "[275] Loss: 0.132  \t  val loss A.: 0.131\n",
      "[276] Loss: 0.134  \t  val loss A.: 0.113\n",
      "[277] Loss: 0.129  \t  val loss A.: 0.119\n",
      "[278] Loss: 0.133  \t  val loss A.: 0.133\n",
      "[279] Loss: 0.130  \t  val loss A.: 0.112\n",
      "[280] Loss: 0.127  \t  val loss A.: 0.123\n",
      "[281] Loss: 0.138  \t  val loss A.: 0.130\n",
      "[282] Loss: 0.119  \t  val loss A.: 0.114\n",
      "[283] Loss: 0.137  \t  val loss A.: 0.098\n",
      "[284] Loss: 0.134  \t  val loss A.: 0.117\n",
      "[285] Loss: 0.124  \t  val loss A.: 0.138\n",
      "[286] Loss: 0.130  \t  val loss A.: 0.124\n",
      "[287] Loss: 0.141  \t  val loss A.: 0.115\n",
      "[288] Loss: 0.129  \t  val loss A.: 0.138\n",
      "[289] Loss: 0.131  \t  val loss A.: 0.121\n",
      "[290] Loss: 0.133  \t  val loss A.: 0.107\n",
      "[291] Loss: 0.136  \t  val loss A.: 0.104\n",
      "[292] Loss: 0.123  \t  val loss A.: 0.094\n",
      "[293] Loss: 0.137  \t  val loss A.: 0.103\n",
      "[294] Loss: 0.126  \t  val loss A.: 0.113\n",
      "[295] Loss: 0.132  \t  val loss A.: 0.137\n",
      "[296] Loss: 0.127  \t  val loss A.: 0.142\n",
      "[297] Loss: 0.131  \t  val loss A.: 0.090\n",
      "[298] Loss: 0.131  \t  val loss A.: 0.105\n",
      "[299] Loss: 0.130  \t  val loss A.: 0.131\n"
     ]
    }
   ],
   "source": [
    "# Start a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set up training\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run the training for some iterations\n",
    "for it in range(300):\n",
    "    sess.run(switch_train_op)\n",
    "\n",
    "    loss_vals = []\n",
    "    # Run 10 training iterations and 1 validation iteration\n",
    "    for i in range(10):\n",
    "        loss_val, _ = sess.run([loss, opt])\n",
    "        loss_vals.append(loss_val)\n",
    "    \n",
    "    sess.run(switch_valid_op)\n",
    "    loss_val = sess.run(loss)\n",
    "\n",
    "    # Let's update tensorboard\n",
    "    summary_writer.add_summary( sess.run(merged_summary, {'loss:0': np.mean(loss_vals), 'val_loss:0': loss_val}), it )\n",
    "    print('[%3d] Loss: %0.3f  \\t  val loss A.: %0.3f'%(it, np.mean(loss_vals), loss_val))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation\n",
    "### Compute the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean absolute difference', 8.1669874)\n"
     ]
    }
   ],
   "source": [
    "total_lbl, total_cor = np.zeros(6)+1e-10, np.zeros(6)\n",
    "I0 = tf.placeholder(tf.float32, shape=(1, None, None, 3))\n",
    "LR = tf.layers.average_pooling2d(I0, 5, 4, padding='SAME', name='image_lr')\n",
    "\n",
    "losses = []\n",
    "for it in tf.python_io.tf_record_iterator('valid.tfrecord'):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(it)\n",
    "    I = np.frombuffer(example.features.feature['image_raw'].bytes_list.value[0], dtype=np.uint8).reshape(256, 256, 3)\n",
    "    L = np.frombuffer(example.features.feature['label_raw'].bytes_list.value[0], dtype=np.uint8).reshape(256, 256)\n",
    "    \n",
    "    lr_val = sess.run(LR, {I0: I[None]})\n",
    "    r = sess.run('output:0', {'image_lr:0':lr_val, 'label:0': L[None]})[0]\n",
    "    losses.append(np.mean(np.abs(r.astype(np.float32)-I)))\n",
    "print( 'Mean absolute difference', np.mean(losses) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Save Model\n",
    "Please note that we also want you to turn in your ipynb for this assignment.  Zip up the ipynb along with the tfg for your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util.save('assignment8.tfg', session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
