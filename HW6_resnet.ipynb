{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "In this homework we will build deeper networks with 20 or more layers.\n",
    "\n",
    "Development notes: \n",
    "\n",
    "v1: Deep net without batch norm: hard and long to train, bad accuracy on test set\n",
    "v2: Deep net with batch norm: much faster to train got best accuracy of 97.1 by throwing compute at it\n",
    "v3: Deep net with resnet structure : TBD\n",
    "\n",
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (12257, 64, 64, 3)\n",
      "Labels shape: (12257,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "# Load the data we are giving you\n",
    "def load(filename, W=64, H=64):\n",
    "    data = np.fromfile(filename, dtype=np.uint8).reshape((-1, W*H*3+1))\n",
    "    images, labels = data[:, :-1].reshape((-1,H,W,3)), data[:, -1]\n",
    "    return images, labels\n",
    "\n",
    "image_data, label_data = load('tux_train.dat')\n",
    "\n",
    "print('Input shape: ' + str(image_data.shape))\n",
    "print('Labels shape: ' + str(label_data.shape))\n",
    "\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define your convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_layer(x, n_out):\n",
    "    \"\"\"\n",
    "    Compute a resnet layer given input and output conditions. Note I am using tf.contrib.layers to create a layer with\n",
    "    ReLU activation and Xavier Initialization. Using a linear projection to match shapes.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input vector\n",
    "    num_outputs -- Number of outputs that the layer will have \n",
    "    Return:\n",
    "    H_x -- which is f_x + x as per the resnet paper by Kaiming He\n",
    "    \"\"\"\n",
    "    F_x= tf.contrib.layers.conv2d(inputs=x,num_outputs=n_out,kernel_size=[3, 3],\n",
    "         weights_regularizer=tf.nn.l2_loss,stride=1)\n",
    "    x= tf.contrib.layers.conv2d(inputs=x,num_outputs=n_out,kernel_size=[3, 3],\n",
    "         weights_regularizer=tf.nn.l2_loss,stride=1,activation_fn=None)\n",
    "    H_x= F_x + x    \n",
    "#     op= tf.contrib.layers.conv2d(inputs=H_x,num_outputs,kernel_size=[3, 3],\n",
    "#          weights_regularizer=tf.nn.l2_loss,stride=1)\n",
    "    \n",
    "    return H_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/model/batch_normalization/batchnorm/add_1:0\", shape=(?, 64, 64, 15), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_2/batchnorm/add_1:0\", shape=(?, 64, 64, 15), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_3/batchnorm/add_1:0\", shape=(?, 64, 64, 15), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_4/batchnorm/add_1:0\", shape=(?, 64, 64, 15), dtype=float32)\n",
      "Tensor(\"model/model/pool1/MaxPool:0\", shape=(?, 32, 32, 15), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_5/batchnorm/add_1:0\", shape=(?, 32, 32, 20), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_6/batchnorm/add_1:0\", shape=(?, 32, 32, 20), dtype=float32)\n",
      "Tensor(\"model/model/pool2/MaxPool:0\", shape=(?, 16, 16, 20), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_7/batchnorm/add_1:0\", shape=(?, 16, 16, 25), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_8/batchnorm/add_1:0\", shape=(?, 16, 16, 25), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_9/batchnorm/add_1:0\", shape=(?, 16, 16, 25), dtype=float32)\n",
      "Tensor(\"model/model/pool3/MaxPool:0\", shape=(?, 8, 8, 25), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_10/batchnorm/add_1:0\", shape=(?, 8, 8, 35), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_11/batchnorm/add_1:0\", shape=(?, 8, 8, 35), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_12/batchnorm/add_1:0\", shape=(?, 8, 8, 35), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_13/batchnorm/add_1:0\", shape=(?, 8, 8, 35), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_14/batchnorm/add_1:0\", shape=(?, 8, 8, 35), dtype=float32)\n",
      "Tensor(\"model/model/pool4/MaxPool:0\", shape=(?, 4, 4, 35), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_15/batchnorm/add_1:0\", shape=(?, 4, 4, 60), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_16/batchnorm/add_1:0\", shape=(?, 4, 4, 60), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_17/batchnorm/add_1:0\", shape=(?, 4, 4, 60), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_18/batchnorm/add_1:0\", shape=(?, 4, 4, 60), dtype=float32)\n",
      "Tensor(\"model/model/batch_normalization_19/batchnorm/add_1:0\", shape=(?, 4, 4, 60), dtype=float32)\n",
      "Tensor(\"model/model/pool5/MaxPool:0\", shape=(?, 1, 1, 60), dtype=float32)\n",
      "Tensor(\"model/model/conv20/BiasAdd:0\", shape=(?, 1, 1, 6), dtype=float32)\n",
      "Total number of variables used  460976 / 500000\n"
     ]
    }
   ],
   "source": [
    "# Lets clear the tensorflow graph, so that you don't have to restart the notebook every time you change the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Set up your input placeholder\n",
    "inputs = tf.placeholder(tf.float32, (None,64,64,3))\n",
    "\n",
    "# Set up your input placeholder\n",
    "training = tf.placeholder_with_default(False, (), name='training')\n",
    "\n",
    "# Step 1: Augment the training data\n",
    "def data_augmentation(I):\n",
    "    I= tf.image.random_flip_left_right(I)\n",
    "    return I\n",
    "\n",
    "# map_fn applies data_augmentation independently for each image in the batch, since we are not croping \n",
    "# let's apply the augmentation before whitening, it does make evaluation easier\n",
    "aug_input = tf.map_fn(data_augmentation, inputs)\n",
    "\n",
    "# During evaluation we don't want data augmentation\n",
    "eval_inputs = tf.identity(aug_input, name='inputs')\n",
    "\n",
    "# Whenever you deal with image data it's important to mean center it first and subtract the standard deviation\n",
    "white_inputs = (eval_inputs - 100.) / 72.\n",
    "\n",
    "# Set up your label placeholders\n",
    "labels = tf.placeholder(tf.int64, (None), name='labels')\n",
    "# lrate= tf.placeholder(tf.float32,name='learning_rate')\n",
    "\n",
    "with tf.name_scope('model'), tf.variable_scope('model'):   \n",
    "    # Step 4: Add residual connections\n",
    "    #  For simplicity you do not need to add a residual connection to every layer, but add them to at least \n",
    "    #      half of your layers\n",
    "    #  Train your model (you should see it converge even faster now).\n",
    "    \n",
    "    h = white_inputs\n",
    "    count=0\n",
    "    for i in range(4):\n",
    "        name= 'conv'+ str(i+1)\n",
    "        h= resnet_layer(h,15)\n",
    "        h= tf.layers.batch_normalization(h,training=training)\n",
    "        print(h)\n",
    "        count+=1\n",
    "    h = tf.contrib.layers.max_pool2d(inputs=h, kernel_size=[2,2], stride=2, scope='pool1')\n",
    "    print(h)\n",
    "    \n",
    "    for i in range(count,count+2):\n",
    "        name= 'conv'+ str(i+1)\n",
    "        h = resnet_layer(h,20)\n",
    "        h= tf.layers.batch_normalization(h,training=training)\n",
    "        print(h)\n",
    "        count+=1\n",
    "    \n",
    "    h = tf.contrib.layers.max_pool2d(inputs=h, kernel_size=[2,2], stride=2, scope='pool2')\n",
    "    print(h)\n",
    "    \n",
    "    for i in range(count,count+3):\n",
    "        name= 'conv'+ str(i+1)\n",
    "        h = resnet_layer(h,25)\n",
    "        h= tf.layers.batch_normalization(h,training=training)\n",
    "        print(h)\n",
    "        count+=1\n",
    "    h = tf.contrib.layers.max_pool2d(inputs=h, kernel_size=[2,2], stride=2, scope='pool3')\n",
    "    print(h)\n",
    "    \n",
    "    for i in range(count,count+5):\n",
    "        name= 'conv'+ str(i+1)\n",
    "        h = resnet_layer(h,35)\n",
    "        h= tf.layers.batch_normalization(h,training=training)\n",
    "        print(h)\n",
    "        count+=1\n",
    "    \n",
    "    h = tf.contrib.layers.max_pool2d(inputs=h, kernel_size=[2,2], stride=2, scope='pool4')\n",
    "    print(h)\n",
    "    \n",
    "    for i in range(count,count+5):\n",
    "        name= 'conv'+ str(i+1)\n",
    "        h = resnet_layer(h,60)\n",
    "        h= tf.layers.batch_normalization(h,training=training)\n",
    "        print(h)\n",
    "        count+=1\n",
    "    \n",
    "    h = tf.contrib.layers.max_pool2d(inputs=h, kernel_size=[3,3], stride=2, scope='pool5')\n",
    "    print(h)\n",
    "    \n",
    "    h= tf.contrib.layers.conv2d(inputs=h,num_outputs=6,kernel_size=[1,1],\n",
    "        weights_regularizer=tf.nn.l2_loss,stride=1,activation_fn=None, scope='conv20')\n",
    "    print(h)    \n",
    "    # The input here should be a   None x 1 x 1 x 6   tensor\n",
    "    h = tf.contrib.layers.flatten(h,scope='out')\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=labels))\n",
    "\n",
    "output = tf.identity(h, name='output')\n",
    "\n",
    "regularization_loss = tf.losses.get_regularization_loss()\n",
    "\n",
    "# Let's weight the regularization loss down, otherwise it will hurt the model performance\n",
    "# You can tune this weight if you wish\n",
    "total_loss = loss + 1e-6 * regularization_loss\n",
    "\n",
    "# create an optimizer\n",
    "# NOTE: you might have to play with the learning rate as you try out \n",
    "# batch_normalization (0.001 might work well without BN, 0.1 with, 0.001 for resnets)\n",
    "optimizer = tf.train.AdamOptimizer(0.001, 0.9, 0.999)\n",
    "\n",
    "# use that optimizer on your loss function (control_dependencies makes sure any \n",
    "# batch_norm parameters are properly updated)\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    opt = optimizer.minimize(total_loss)\n",
    "correct = tf.equal(tf.argmax(output, 1), labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "print( \"Total number of variables used \", np.sum([v.get_shape().num_elements() for v in tf.trainable_variables()]), '/', 500000 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training\n",
    "\n",
    "Training might take up to 20 min depending on your architecture (and if you have a GPU or not). A network without BN will train much slower, but try it first anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0] Accuracy: 0.886  \t  Loss: 0.353  \t  validation accuracy: 0.825\n",
      "[  1] Accuracy: 0.933  \t  Loss: 0.204  \t  validation accuracy: 0.913\n",
      "[  2] Accuracy: 0.948  \t  Loss: 0.153  \t  validation accuracy: 0.889\n",
      "[  3] Accuracy: 0.957  \t  Loss: 0.133  \t  validation accuracy: 0.898\n",
      "[  4] Accuracy: 0.960  \t  Loss: 0.116  \t  validation accuracy: 0.925\n",
      "[  5] Accuracy: 0.965  \t  Loss: 0.105  \t  validation accuracy: 0.931\n",
      "[  6] Accuracy: 0.968  \t  Loss: 0.097  \t  validation accuracy: 0.915\n",
      "[  7] Accuracy: 0.972  \t  Loss: 0.083  \t  validation accuracy: 0.937\n",
      "[  8] Accuracy: 0.972  \t  Loss: 0.082  \t  validation accuracy: 0.900\n",
      "[  9] Accuracy: 0.974  \t  Loss: 0.071  \t  validation accuracy: 0.938\n",
      "[ 10] Accuracy: 0.980  \t  Loss: 0.061  \t  validation accuracy: 0.934\n",
      "[ 11] Accuracy: 0.977  \t  Loss: 0.067  \t  validation accuracy: 0.953\n",
      "[ 12] Accuracy: 0.978  \t  Loss: 0.069  \t  validation accuracy: 0.944\n",
      "[ 13] Accuracy: 0.981  \t  Loss: 0.059  \t  validation accuracy: 0.930\n",
      "[ 14] Accuracy: 0.978  \t  Loss: 0.062  \t  validation accuracy: 0.914\n",
      "[ 15] Accuracy: 0.984  \t  Loss: 0.053  \t  validation accuracy: 0.917\n",
      "[ 16] Accuracy: 0.981  \t  Loss: 0.055  \t  validation accuracy: 0.936\n",
      "[ 17] Accuracy: 0.987  \t  Loss: 0.040  \t  validation accuracy: 0.953\n",
      "[ 18] Accuracy: 0.986  \t  Loss: 0.040  \t  validation accuracy: 0.935\n",
      "[ 19] Accuracy: 0.986  \t  Loss: 0.045  \t  validation accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "image_val, label_val = load('tux_val.dat')\n",
    "# Batch size\n",
    "BS = 32\n",
    "\n",
    "# Start a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set up training\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# An epoch is a single pass over the training data\n",
    "creation=0\n",
    "for epoch in range(20):\n",
    "    # Let's shuffle the data every epoch\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(image_data)\n",
    "    np.random.seed(epoch)\n",
    "    np.random.shuffle(label_data)\n",
    "    # Go through the entire dataset once\n",
    "    accuracy_vals, loss_vals = [], []\n",
    "    for i in range(0, image_data.shape[0]-BS+1, BS):\n",
    "        # Train a single batch\n",
    "        batch_images, batch_labels = image_data[i:i+BS], label_data[i:i+BS]\n",
    "        accuracy_val, loss_val, _ = sess.run([accuracy, total_loss, opt], feed_dict={inputs: batch_images, labels: batch_labels, training:True})\n",
    "        accuracy_vals.append(accuracy_val)\n",
    "        loss_vals.append(loss_val)\n",
    "\n",
    "    val_correct = []\n",
    "    for i in range(0, image_val.shape[0], BS):\n",
    "        batch_images, batch_labels = image_val[i:i+BS], label_val[i:i+BS]\n",
    "        val_correct.extend( sess.run(correct, feed_dict={eval_inputs: batch_images, labels: batch_labels}) )\n",
    "    print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f  \\t  validation accuracy: %0.3f'%(epoch, np.mean(accuracy_vals), np.mean(loss_vals), np.mean(val_correct)))\n",
    "    \n",
    "#     if creation==0:\n",
    "#         present_cal_acc= np.mean(val_correct)\n",
    "#         creation+=1\n",
    "#     if present_cal_acc+0.005 < np.mean(val_correct):\n",
    "#         present_cal_acc= np.mean(val_correct)\n",
    "#         print(\"saving\")\n",
    "#         util.save('assignment6_v3_best.tfg', session=sess)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the valiation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3912, 64, 64, 3)\n",
      "Labels shape: (3912,)\n",
      "ConvNet Validation Accuracy:  0.955010224949\n"
     ]
    }
   ],
   "source": [
    "image_val, label_val = load('tux_val.dat')\n",
    "\n",
    "print('Input shape: ' + str(image_val.shape))\n",
    "print('Labels shape: ' + str(label_val.shape))\n",
    "\n",
    "val_correct = []\n",
    "for i in range(0, image_val.shape[0], BS):\n",
    "    batch_images, batch_labels = image_val[i:i+BS], label_val[i:i+BS]\n",
    "    val_correct.extend( sess.run(correct, feed_dict={eval_inputs: batch_images, labels: batch_labels}) )\n",
    "print(\"ConvNet Validation Accuracy: \", np.mean(val_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Save Model\n",
    "Please note that we also want you to turn in your ipynb for this assignment.  Zip up the ipynb along with the tfg for your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# util.save('assignment6_v3.tfg', session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 (optional): See your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "# Show the current graph\n",
    "util.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
